{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assg5_FMIT2021005.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4rQaTgNkJ1_",
        "outputId": "26addd96-66a6-40c3-851b-11143a0e8cc6"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0asbkDvknKP",
        "outputId": "2fd50d1a-798b-4ef2-86df-4d04655e11ad"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atDBdZ0RksZR",
        "outputId": "f1d81cca-2189-46cb-9596-73f8455dad23"
      },
      "source": [
        "from nltk.book import *\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3l-YizolCcq"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import wordnet as wn\n",
        "from operator import itemgetter\n",
        "from timeit import Timer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2a1LGjIlU_V"
      },
      "source": [
        "1. Find out more about sequence objects using Python's help facility. In the interpreter, type help(str), help(list) , and help(tuple) . This will give you a full list of the functions supported by each type. Some functions have special names flanked with underscore; as the help documentation shows, each such function corresponds to something more familiar. For example x.__getitem__(y) is just a long-winded way of saying x[y] ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T9D8MfHlYao",
        "outputId": "143ed2f8-e5be-437e-973b-f94faa092f03"
      },
      "source": [
        "help(str)\n",
        "help(list)\n",
        "help(tuple)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class str in module builtins:\n",
            "\n",
            "class str(object)\n",
            " |  str(object='') -> str\n",
            " |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
            " |  \n",
            " |  Create a new string object from the given object. If encoding or\n",
            " |  errors is specified, then the object must expose a data buffer\n",
            " |  that will be decoded using the given encoding and error handler.\n",
            " |  Otherwise, returns the result of object.__str__() (if defined)\n",
            " |  or repr(object).\n",
            " |  encoding defaults to sys.getdefaultencoding().\n",
            " |  errors defaults to 'strict'.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self, value, /)\n",
            " |      Return self+value.\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      Return key in self.\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __format__(self, format_spec, /)\n",
            " |      Return a formatted version of the string as described by format_spec.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __getitem__(self, key, /)\n",
            " |      Return self[key].\n",
            " |  \n",
            " |  __getnewargs__(...)\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __hash__(self, /)\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __mod__(self, value, /)\n",
            " |      Return self%value.\n",
            " |  \n",
            " |  __mul__(self, value, /)\n",
            " |      Return self*value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __rmod__(self, value, /)\n",
            " |      Return value%self.\n",
            " |  \n",
            " |  __rmul__(self, value, /)\n",
            " |      Return value*self.\n",
            " |  \n",
            " |  __sizeof__(self, /)\n",
            " |      Return the size of the string in memory, in bytes.\n",
            " |  \n",
            " |  __str__(self, /)\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  capitalize(self, /)\n",
            " |      Return a capitalized version of the string.\n",
            " |      \n",
            " |      More specifically, make the first character have upper case and the rest lower\n",
            " |      case.\n",
            " |  \n",
            " |  casefold(self, /)\n",
            " |      Return a version of the string suitable for caseless comparisons.\n",
            " |  \n",
            " |  center(self, width, fillchar=' ', /)\n",
            " |      Return a centered string of length width.\n",
            " |      \n",
            " |      Padding is done using the specified fill character (default is a space).\n",
            " |  \n",
            " |  count(...)\n",
            " |      S.count(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the number of non-overlapping occurrences of substring sub in\n",
            " |      string S[start:end].  Optional arguments start and end are\n",
            " |      interpreted as in slice notation.\n",
            " |  \n",
            " |  encode(self, /, encoding='utf-8', errors='strict')\n",
            " |      Encode the string using the codec registered for encoding.\n",
            " |      \n",
            " |      encoding\n",
            " |        The encoding in which to encode the string.\n",
            " |      errors\n",
            " |        The error handling scheme to use for encoding errors.\n",
            " |        The default is 'strict' meaning that encoding errors raise a\n",
            " |        UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and\n",
            " |        'xmlcharrefreplace' as well as any other name registered with\n",
            " |        codecs.register_error that can handle UnicodeEncodeErrors.\n",
            " |  \n",
            " |  endswith(...)\n",
            " |      S.endswith(suffix[, start[, end]]) -> bool\n",
            " |      \n",
            " |      Return True if S ends with the specified suffix, False otherwise.\n",
            " |      With optional start, test S beginning at that position.\n",
            " |      With optional end, stop comparing S at that position.\n",
            " |      suffix can also be a tuple of strings to try.\n",
            " |  \n",
            " |  expandtabs(self, /, tabsize=8)\n",
            " |      Return a copy where all tab characters are expanded using spaces.\n",
            " |      \n",
            " |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
            " |  \n",
            " |  find(...)\n",
            " |      S.find(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the lowest index in S where substring sub is found,\n",
            " |      such that sub is contained within S[start:end].  Optional\n",
            " |      arguments start and end are interpreted as in slice notation.\n",
            " |      \n",
            " |      Return -1 on failure.\n",
            " |  \n",
            " |  format(...)\n",
            " |      S.format(*args, **kwargs) -> str\n",
            " |      \n",
            " |      Return a formatted version of S, using substitutions from args and kwargs.\n",
            " |      The substitutions are identified by braces ('{' and '}').\n",
            " |  \n",
            " |  format_map(...)\n",
            " |      S.format_map(mapping) -> str\n",
            " |      \n",
            " |      Return a formatted version of S, using substitutions from mapping.\n",
            " |      The substitutions are identified by braces ('{' and '}').\n",
            " |  \n",
            " |  index(...)\n",
            " |      S.index(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the lowest index in S where substring sub is found, \n",
            " |      such that sub is contained within S[start:end].  Optional\n",
            " |      arguments start and end are interpreted as in slice notation.\n",
            " |      \n",
            " |      Raises ValueError when the substring is not found.\n",
            " |  \n",
            " |  isalnum(self, /)\n",
            " |      Return True if the string is an alpha-numeric string, False otherwise.\n",
            " |      \n",
            " |      A string is alpha-numeric if all characters in the string are alpha-numeric and\n",
            " |      there is at least one character in the string.\n",
            " |  \n",
            " |  isalpha(self, /)\n",
            " |      Return True if the string is an alphabetic string, False otherwise.\n",
            " |      \n",
            " |      A string is alphabetic if all characters in the string are alphabetic and there\n",
            " |      is at least one character in the string.\n",
            " |  \n",
            " |  isascii(self, /)\n",
            " |      Return True if all characters in the string are ASCII, False otherwise.\n",
            " |      \n",
            " |      ASCII characters have code points in the range U+0000-U+007F.\n",
            " |      Empty string is ASCII too.\n",
            " |  \n",
            " |  isdecimal(self, /)\n",
            " |      Return True if the string is a decimal string, False otherwise.\n",
            " |      \n",
            " |      A string is a decimal string if all characters in the string are decimal and\n",
            " |      there is at least one character in the string.\n",
            " |  \n",
            " |  isdigit(self, /)\n",
            " |      Return True if the string is a digit string, False otherwise.\n",
            " |      \n",
            " |      A string is a digit string if all characters in the string are digits and there\n",
            " |      is at least one character in the string.\n",
            " |  \n",
            " |  isidentifier(self, /)\n",
            " |      Return True if the string is a valid Python identifier, False otherwise.\n",
            " |      \n",
            " |      Use keyword.iskeyword() to test for reserved identifiers such as \"def\" and\n",
            " |      \"class\".\n",
            " |  \n",
            " |  islower(self, /)\n",
            " |      Return True if the string is a lowercase string, False otherwise.\n",
            " |      \n",
            " |      A string is lowercase if all cased characters in the string are lowercase and\n",
            " |      there is at least one cased character in the string.\n",
            " |  \n",
            " |  isnumeric(self, /)\n",
            " |      Return True if the string is a numeric string, False otherwise.\n",
            " |      \n",
            " |      A string is numeric if all characters in the string are numeric and there is at\n",
            " |      least one character in the string.\n",
            " |  \n",
            " |  isprintable(self, /)\n",
            " |      Return True if the string is printable, False otherwise.\n",
            " |      \n",
            " |      A string is printable if all of its characters are considered printable in\n",
            " |      repr() or if it is empty.\n",
            " |  \n",
            " |  isspace(self, /)\n",
            " |      Return True if the string is a whitespace string, False otherwise.\n",
            " |      \n",
            " |      A string is whitespace if all characters in the string are whitespace and there\n",
            " |      is at least one character in the string.\n",
            " |  \n",
            " |  istitle(self, /)\n",
            " |      Return True if the string is a title-cased string, False otherwise.\n",
            " |      \n",
            " |      In a title-cased string, upper- and title-case characters may only\n",
            " |      follow uncased characters and lowercase characters only cased ones.\n",
            " |  \n",
            " |  isupper(self, /)\n",
            " |      Return True if the string is an uppercase string, False otherwise.\n",
            " |      \n",
            " |      A string is uppercase if all cased characters in the string are uppercase and\n",
            " |      there is at least one cased character in the string.\n",
            " |  \n",
            " |  join(self, iterable, /)\n",
            " |      Concatenate any number of strings.\n",
            " |      \n",
            " |      The string whose method is called is inserted in between each given string.\n",
            " |      The result is returned as a new string.\n",
            " |      \n",
            " |      Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'\n",
            " |  \n",
            " |  ljust(self, width, fillchar=' ', /)\n",
            " |      Return a left-justified string of length width.\n",
            " |      \n",
            " |      Padding is done using the specified fill character (default is a space).\n",
            " |  \n",
            " |  lower(self, /)\n",
            " |      Return a copy of the string converted to lowercase.\n",
            " |  \n",
            " |  lstrip(self, chars=None, /)\n",
            " |      Return a copy of the string with leading whitespace removed.\n",
            " |      \n",
            " |      If chars is given and not None, remove characters in chars instead.\n",
            " |  \n",
            " |  partition(self, sep, /)\n",
            " |      Partition the string into three parts using the given separator.\n",
            " |      \n",
            " |      This will search for the separator in the string.  If the separator is found,\n",
            " |      returns a 3-tuple containing the part before the separator, the separator\n",
            " |      itself, and the part after it.\n",
            " |      \n",
            " |      If the separator is not found, returns a 3-tuple containing the original string\n",
            " |      and two empty strings.\n",
            " |  \n",
            " |  replace(self, old, new, count=-1, /)\n",
            " |      Return a copy with all occurrences of substring old replaced by new.\n",
            " |      \n",
            " |        count\n",
            " |          Maximum number of occurrences to replace.\n",
            " |          -1 (the default value) means replace all occurrences.\n",
            " |      \n",
            " |      If the optional argument count is given, only the first count occurrences are\n",
            " |      replaced.\n",
            " |  \n",
            " |  rfind(...)\n",
            " |      S.rfind(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the highest index in S where substring sub is found,\n",
            " |      such that sub is contained within S[start:end].  Optional\n",
            " |      arguments start and end are interpreted as in slice notation.\n",
            " |      \n",
            " |      Return -1 on failure.\n",
            " |  \n",
            " |  rindex(...)\n",
            " |      S.rindex(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the highest index in S where substring sub is found,\n",
            " |      such that sub is contained within S[start:end].  Optional\n",
            " |      arguments start and end are interpreted as in slice notation.\n",
            " |      \n",
            " |      Raises ValueError when the substring is not found.\n",
            " |  \n",
            " |  rjust(self, width, fillchar=' ', /)\n",
            " |      Return a right-justified string of length width.\n",
            " |      \n",
            " |      Padding is done using the specified fill character (default is a space).\n",
            " |  \n",
            " |  rpartition(self, sep, /)\n",
            " |      Partition the string into three parts using the given separator.\n",
            " |      \n",
            " |      This will search for the separator in the string, starting at the end. If\n",
            " |      the separator is found, returns a 3-tuple containing the part before the\n",
            " |      separator, the separator itself, and the part after it.\n",
            " |      \n",
            " |      If the separator is not found, returns a 3-tuple containing two empty strings\n",
            " |      and the original string.\n",
            " |  \n",
            " |  rsplit(self, /, sep=None, maxsplit=-1)\n",
            " |      Return a list of the words in the string, using sep as the delimiter string.\n",
            " |      \n",
            " |        sep\n",
            " |          The delimiter according which to split the string.\n",
            " |          None (the default value) means split according to any whitespace,\n",
            " |          and discard empty strings from the result.\n",
            " |        maxsplit\n",
            " |          Maximum number of splits to do.\n",
            " |          -1 (the default value) means no limit.\n",
            " |      \n",
            " |      Splits are done starting at the end of the string and working to the front.\n",
            " |  \n",
            " |  rstrip(self, chars=None, /)\n",
            " |      Return a copy of the string with trailing whitespace removed.\n",
            " |      \n",
            " |      If chars is given and not None, remove characters in chars instead.\n",
            " |  \n",
            " |  split(self, /, sep=None, maxsplit=-1)\n",
            " |      Return a list of the words in the string, using sep as the delimiter string.\n",
            " |      \n",
            " |      sep\n",
            " |        The delimiter according which to split the string.\n",
            " |        None (the default value) means split according to any whitespace,\n",
            " |        and discard empty strings from the result.\n",
            " |      maxsplit\n",
            " |        Maximum number of splits to do.\n",
            " |        -1 (the default value) means no limit.\n",
            " |  \n",
            " |  splitlines(self, /, keepends=False)\n",
            " |      Return a list of the lines in the string, breaking at line boundaries.\n",
            " |      \n",
            " |      Line breaks are not included in the resulting list unless keepends is given and\n",
            " |      true.\n",
            " |  \n",
            " |  startswith(...)\n",
            " |      S.startswith(prefix[, start[, end]]) -> bool\n",
            " |      \n",
            " |      Return True if S starts with the specified prefix, False otherwise.\n",
            " |      With optional start, test S beginning at that position.\n",
            " |      With optional end, stop comparing S at that position.\n",
            " |      prefix can also be a tuple of strings to try.\n",
            " |  \n",
            " |  strip(self, chars=None, /)\n",
            " |      Return a copy of the string with leading and trailing whitespace removed.\n",
            " |      \n",
            " |      If chars is given and not None, remove characters in chars instead.\n",
            " |  \n",
            " |  swapcase(self, /)\n",
            " |      Convert uppercase characters to lowercase and lowercase characters to uppercase.\n",
            " |  \n",
            " |  title(self, /)\n",
            " |      Return a version of the string where each word is titlecased.\n",
            " |      \n",
            " |      More specifically, words start with uppercased characters and all remaining\n",
            " |      cased characters have lower case.\n",
            " |  \n",
            " |  translate(self, table, /)\n",
            " |      Replace each character in the string using the given translation table.\n",
            " |      \n",
            " |        table\n",
            " |          Translation table, which must be a mapping of Unicode ordinals to\n",
            " |          Unicode ordinals, strings, or None.\n",
            " |      \n",
            " |      The table must implement lookup/indexing via __getitem__, for instance a\n",
            " |      dictionary or list.  If this operation raises LookupError, the character is\n",
            " |      left untouched.  Characters mapped to None are deleted.\n",
            " |  \n",
            " |  upper(self, /)\n",
            " |      Return a copy of the string converted to uppercase.\n",
            " |  \n",
            " |  zfill(self, width, /)\n",
            " |      Pad a numeric string with zeros on the left, to fill a field of the given width.\n",
            " |      \n",
            " |      The string is never truncated.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  maketrans(x, y=None, z=None, /)\n",
            " |      Return a translation table usable for str.translate().\n",
            " |      \n",
            " |      If there is only one argument, it must be a dictionary mapping Unicode\n",
            " |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
            " |      Character keys will be then converted to ordinals.\n",
            " |      If there are two arguments, they must be strings of equal length, and\n",
            " |      in the resulting dictionary, each character in x will be mapped to the\n",
            " |      character at the same position in y. If there is a third argument, it\n",
            " |      must be a string, whose characters will be mapped to None in the result.\n",
            "\n",
            "Help on class list in module builtins:\n",
            "\n",
            "class list(object)\n",
            " |  list(iterable=(), /)\n",
            " |  \n",
            " |  Built-in mutable sequence.\n",
            " |  \n",
            " |  If no argument is given, the constructor creates a new empty list.\n",
            " |  The argument must be an iterable if specified.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self, value, /)\n",
            " |      Return self+value.\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      Return key in self.\n",
            " |  \n",
            " |  __delitem__(self, key, /)\n",
            " |      Delete self[key].\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __getitem__(...)\n",
            " |      x.__getitem__(y) <==> x[y]\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __iadd__(self, value, /)\n",
            " |      Implement self+=value.\n",
            " |  \n",
            " |  __imul__(self, value, /)\n",
            " |      Implement self*=value.\n",
            " |  \n",
            " |  __init__(self, /, *args, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __mul__(self, value, /)\n",
            " |      Return self*value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __reversed__(self, /)\n",
            " |      Return a reverse iterator over the list.\n",
            " |  \n",
            " |  __rmul__(self, value, /)\n",
            " |      Return value*self.\n",
            " |  \n",
            " |  __setitem__(self, key, value, /)\n",
            " |      Set self[key] to value.\n",
            " |  \n",
            " |  __sizeof__(self, /)\n",
            " |      Return the size of the list in memory, in bytes.\n",
            " |  \n",
            " |  append(self, object, /)\n",
            " |      Append object to the end of the list.\n",
            " |  \n",
            " |  clear(self, /)\n",
            " |      Remove all items from list.\n",
            " |  \n",
            " |  copy(self, /)\n",
            " |      Return a shallow copy of the list.\n",
            " |  \n",
            " |  count(self, value, /)\n",
            " |      Return number of occurrences of value.\n",
            " |  \n",
            " |  extend(self, iterable, /)\n",
            " |      Extend list by appending elements from the iterable.\n",
            " |  \n",
            " |  index(self, value, start=0, stop=9223372036854775807, /)\n",
            " |      Return first index of value.\n",
            " |      \n",
            " |      Raises ValueError if the value is not present.\n",
            " |  \n",
            " |  insert(self, index, object, /)\n",
            " |      Insert object before index.\n",
            " |  \n",
            " |  pop(self, index=-1, /)\n",
            " |      Remove and return item at index (default last).\n",
            " |      \n",
            " |      Raises IndexError if list is empty or index is out of range.\n",
            " |  \n",
            " |  remove(self, value, /)\n",
            " |      Remove first occurrence of value.\n",
            " |      \n",
            " |      Raises ValueError if the value is not present.\n",
            " |  \n",
            " |  reverse(self, /)\n",
            " |      Reverse *IN PLACE*.\n",
            " |  \n",
            " |  sort(self, /, *, key=None, reverse=False)\n",
            " |      Stable sort *IN PLACE*.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __hash__ = None\n",
            "\n",
            "Help on class tuple in module builtins:\n",
            "\n",
            "class tuple(object)\n",
            " |  tuple(iterable=(), /)\n",
            " |  \n",
            " |  Built-in immutable sequence.\n",
            " |  \n",
            " |  If no argument is given, the constructor returns an empty tuple.\n",
            " |  If iterable is specified the tuple is initialized from iterable's items.\n",
            " |  \n",
            " |  If the argument is a tuple, the return value is the same object.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self, value, /)\n",
            " |      Return self+value.\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      Return key in self.\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __getitem__(self, key, /)\n",
            " |      Return self[key].\n",
            " |  \n",
            " |  __getnewargs__(self, /)\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __hash__(self, /)\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __mul__(self, value, /)\n",
            " |      Return self*value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __rmul__(self, value, /)\n",
            " |      Return value*self.\n",
            " |  \n",
            " |  count(self, value, /)\n",
            " |      Return number of occurrences of value.\n",
            " |  \n",
            " |  index(self, value, start=0, stop=9223372036854775807, /)\n",
            " |      Return first index of value.\n",
            " |      \n",
            " |      Raises ValueError if the value is not present.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHzLwtFXlqpV"
      },
      "source": [
        "\n",
        "2. Identify three operations that can be performed on both tuples and lists. Identify three list operations that cannot be performed on tuples. Name a context where using a list instead of a tuple generates a Python error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmC2NOEUl4rR"
      },
      "source": [
        "# https://docs.python.org/3/library/stdtypes.html#typesseq\n",
        "\n",
        "exp_list = [\"Python\",\"IoT\",\"NLP\"]\n",
        "exp_tuple = \"Python\",\"IoT\",\"NLP\"\n",
        "\n",
        "# Common operations\n",
        "\"Python\" in exp_list              # in\n",
        "\"Python\" not in exp_tuple         # not in\n",
        "\n",
        "exp_list[0]                         # subsciption\n",
        "exp_tuple[1:]                       # slicing\n",
        "\n",
        "len(exp_list)                       # length\n",
        "len(exp_tuple)\n",
        "\n",
        "min(exp_list)                       # smallest item\n",
        "max(exp_tuple)                      # largest item\n",
        "\n",
        "exp_list.index(\"IoT\")          # index of the first occurrence\n",
        "exp_tuple.index(\"NLP\", 1)    # index of the first occurrence (at or after index 1)\n",
        "\n",
        "# List operations(mutable) that cannot be performed on tuples\n",
        "exp_list[0] = \"Python\"\n",
        "del exp_list[1]\n",
        "exp_list.append('understanding')\n",
        "exp_list.insert(1, 'Language')\n",
        "exp_list.pop()\n",
        "exp_list.remove(\"Language\")\n",
        "exp_list.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfpQHrBnmSZ6"
      },
      "source": [
        "\n",
        "tuple_o = \"NLP\", ['N', 'AE1', 'CH', 'ER0', 'AH0', 'L']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdlFg9K3mU3_"
      },
      "source": [
        "\n",
        "3. Find out how to create a tuple consisting of a single item. There are at least two ways to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otxn_y1dmYNv",
        "outputId": "1d8eb947-2bd9-4ba0-f945-b62cc73ee030"
      },
      "source": [
        "tuple1 = \"Python\",\n",
        "tuple2 = tuple([\"Python\"])\n",
        "tuple0 = tuple([\"Python\",\"IoT\"])\n",
        "print(type(tuple1))\n",
        "print(type(tuple2))\n",
        "print(type(tuple0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n",
            "<class 'tuple'>\n",
            "<class 'tuple'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m74cPYVLme-1"
      },
      "source": [
        "\n",
        "4. Create a list words = ['is', 'NLP', 'fun', '?'] . Use a series of assignment statements (e.g. words[1] = words[2] ) and a temporary variable tmp to transform this list into the list ['NLP', 'is', 'fun', '!'] . Now do the same transformation using tuple assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEXbnvMamhiZ",
        "outputId": "6ec95104-5d74-4f5f-d95c-81bd948712ed"
      },
      "source": [
        "words = ['is', 'NLP', 'fun', '?']\n",
        "words[0], words[1] = words[1], words[0]     # tmp is not necessary\n",
        "words[-1] = '!'\n",
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'is', 'fun', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYJk10X1mwkU",
        "outputId": "60c39d49-8cf0-4d96-e96b-ab393a7a686d"
      },
      "source": [
        "words_tuple = 'is', 'NLP', 'fun', '?'\n",
        "words_new = words_tuple[1], words_tuple[0], words_tuple[2], '!'\n",
        "list(words_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'is', 'fun', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6apYcYlDmkvQ"
      },
      "source": [
        "\n",
        "5. Read about the built-in comparison function cmp , by typing help(cmp) . How does it differ in behavior from the comparison operators?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojGT2X_em5dJ"
      },
      "source": [
        "\n",
        "Well, cmp() compare the two objects x and y and return an integer according to the outcome. The return value is negative if x < y, zero if x == y and strictly positive if x > y.\n",
        "However, it was deprecated in Python3 and cmp is no longer a built-in comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Ji3ZLtm-5-"
      },
      "source": [
        "\n",
        "6. Does the method for creating a sliding window of n-grams behave correctly for the two limiting cases: n = 1, and n = len(sent) ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlcrdFHlnF0Y",
        "outputId": "40789944-99e1-4f41-a01d-663093333685"
      },
      "source": [
        "sent = [\"Python\",\"is \",\"easy\",\"to\",\"learn\"]\n",
        "n = len(sent)     # n = 1\n",
        "[sent[i:i+n] for i in range(len(sent)-n+1)]\n",
        "# The two boundary cases both works."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Python', 'is ', 'easy', 'to', 'learn']]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaO8Db_unfff"
      },
      "source": [
        "7. We pointed out that when empty strings and empty lists occur in the condition part of an if clause, they evaluate to False . In this case, they are said to be occurring in a Boolean context. Experiment with different kind of non-Boolean expressions in Boolean contexts, and see whether they evaluate as True or False ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8_VMp3Wnl3O",
        "outputId": "24f35358-a65b-46a8-ba66-632b799cd5eb"
      },
      "source": [
        "\n",
        "a = 1\n",
        "if a:\n",
        "    print('True')\n",
        "else:\n",
        "    print('False')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsdlniVPnv8K"
      },
      "source": [
        "True\n",
        "False, None, numeric zero of all types, and empty strings and containers (including strings, tuples, lists, dictionaries, sets and frozensets). All other values are interpreted as true."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbHO4eW8n0f0"
      },
      "source": [
        "\n",
        "8. Use the inequality operators to compare strings, e.g. 'Monty' < 'Python' . What happens when you do 'Z' < 'a' ? Try pairs of strings which have a common prefix, e.g. 'Monty' < 'Montague' . Read up on \"lexicographical sort\" in order to understand what is going on here. Try comparing structured objects, e.g. ('Monty', 1) < ('Monty', 2) . Does this behave as expected?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKpRpoZGn6Z0",
        "outputId": "871e9946-5760-4a1a-8a65-56e2242d861e"
      },
      "source": [
        "print('Monty' < 'Python')\n",
        "\n",
        "print('Z' < 'a')                  # 90('Z') and 97('a') in ASCII\n",
        "\n",
        "print('Monty' < 'Montague')\n",
        "# Lexicographical sort is introduced in Discrete Mathematics and Its Applications\n",
        "# https://en.wikipedia.org/wiki/Lexicographical_order\n",
        "\n",
        "print(('Monty', 1) < ('Monty', 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "False\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wivqU7yn-oN"
      },
      "source": [
        "\n",
        "9. Write code that removes whitespace at the beginning and end of a string, and normalizes whitespace between words to be a single space character.\n",
        "\n",
        "10. do this task using split() and join()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S2XSnuj8oCBg",
        "outputId": "03633845-2076-4095-be9a-3175b845202e"
      },
      "source": [
        "s = ' this is \\n a sample\\t sentence. '\n",
        "' '.join(s.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is a sample sentence.'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oI-S0TQo_UN"
      },
      "source": [
        "11. do this task using regular expression substitutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5N9uC_vAoLIz",
        "outputId": "0577f9d9-a475-4db7-9857-1bc336cd45ec"
      },
      "source": [
        "\n",
        "import re\n",
        "s_ = re.sub(r'^\\s|\\s$', '', s)       # remove whitespace at the beginning and end of a string\n",
        "re.sub(r'\\s+', ' ', s_)              # normalize whitespace between words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is a sample sentence.'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koqNJevgoQwV"
      },
      "source": [
        "\n",
        "12. Write a program to sort words by length. Define a helper function cmp_len which uses the cmp comparison function on word lengths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdRsDUNmoUx-",
        "outputId": "d89a83bf-ee6f-4812-e18f-22d017167d4d"
      },
      "source": [
        "word_list = 'a program to sort words by length'.split()\n",
        "sorted(word_list, key=len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'to', 'by', 'sort', 'words', 'length', 'program']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5RkKuqEpHI6"
      },
      "source": [
        "\n",
        "13. Create a list of words and store it in a variable sent1 . Now assign sent2 = sent1 . Modify one of the items in sent1 and verify that sent2 has changed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6215KvnpLFk",
        "outputId": "cc23d92d-0580-404b-a110-e01e727cd508"
      },
      "source": [
        "\n",
        "sent1 = [\"This\",\"is\",\"a\",\"list\"]\n",
        "sent2 = sent1\n",
        "sent1[3] = 'words'\n",
        "sent2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'words']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj2GgW6Lpl6-"
      },
      "source": [
        "14. Now try the same exercise but instead assign sent2 = sent1[:] . Modify sent1 again and see what happens to sent2 . Explain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF8HPp_OpTSY",
        "outputId": "3e1adc5a-77a9-4f8d-cca9-ff754d410bb5"
      },
      "source": [
        "sent2 = sent1[:]\n",
        "sent1[3] = 'Word'\n",
        "sent2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'words']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjQ53jDjpwN1"
      },
      "source": [
        "15. Now define text1 to be a list of lists of strings (e.g. to represent a text consisting of multiple sentences. Now assign text2 = text1[:] , assign a new value to one of the words, e.g. text1[1][1] = 'Monty' . Check what this did to text2 . Explain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBnOS7WvpxMn",
        "outputId": "9c9af0da-ef56-4454-e4a5-e90ced535772"
      },
      "source": [
        "text1 = [\"Hush, little baby, don't say a word,\".split(),\n",
        "         \"Papa's going to buy you a mockingbird.\".split(),\n",
        "         \"If that mockingbird won't sing,\".split(),\n",
        "         \"Papa's going to buy you a diamond ring.\".split(),\n",
        "         \"If that diamond ring turns to brass,\".split(),\n",
        "         \"Papa's going to buy you a looking-glass.\".split(),\n",
        "         \"If that looking-glass gets broke,\".split(),\n",
        "         \"Papa's going to buy you a billy-goat.\".split(),\n",
        "         \"If that billy-goat runs away,\".split(),\n",
        "         \"Papa's going to buy you another today.\".split()]\n",
        "text2 = text1[:]\n",
        "text1[1][1] = 'Monty'\n",
        "text2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hush,', 'little', 'baby,', \"don't\", 'say', 'a', 'word,'],\n",
              " [\"Papa's\", 'Monty', 'to', 'buy', 'you', 'a', 'mockingbird.'],\n",
              " ['If', 'that', 'mockingbird', \"won't\", 'sing,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'diamond', 'ring.'],\n",
              " ['If', 'that', 'diamond', 'ring', 'turns', 'to', 'brass,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'looking-glass.'],\n",
              " ['If', 'that', 'looking-glass', 'gets', 'broke,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'billy-goat.'],\n",
              " ['If', 'that', 'billy-goat', 'runs', 'away,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'another', 'today.']]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ad6ZcCEp59I"
      },
      "source": [
        "\n",
        "16. Load Python's deepcopy() function (i.e. from copy import deepcopy ), consult its documentation, and test that it makes a fresh copy of any object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F22q1qWzp7Ff",
        "outputId": "a975475b-8dba-4f69-cb63-bedef3ba878e"
      },
      "source": [
        "from copy import deepcopy\n",
        "text3 = deepcopy(text1)\n",
        "text1[1][1] = 'going'\n",
        "text3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hush,', 'little', 'baby,', \"don't\", 'say', 'a', 'word,'],\n",
              " [\"Papa's\", 'Monty', 'to', 'buy', 'you', 'a', 'mockingbird.'],\n",
              " ['If', 'that', 'mockingbird', \"won't\", 'sing,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'diamond', 'ring.'],\n",
              " ['If', 'that', 'diamond', 'ring', 'turns', 'to', 'brass,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'looking-glass.'],\n",
              " ['If', 'that', 'looking-glass', 'gets', 'broke,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'billy-goat.'],\n",
              " ['If', 'that', 'billy-goat', 'runs', 'away,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'another', 'today.']]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DwEnfbwqO8B"
      },
      "source": [
        "17. Initialize an n-by-m list of lists of empty strings using list multiplication, e.g. word_table = [[''] * n] * m . What happens when you set one of its values, e.g. word_table[1][2] = \"hello\" ? Explain why this happens. Now write an expression using range() to construct a list of lists, and show that it does not have this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHbeYgpIqP6d",
        "outputId": "6c06fc7c-f3ca-4764-de9a-2203496dbe1a"
      },
      "source": [
        "m = 4\n",
        "n = 3\n",
        "word_table = [[''] * n] * m\n",
        "word_table[1][2] = \"Heya\"\n",
        "word_table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['', '', 'Heya'], ['', '', 'Heya'], ['', '', 'Heya'], ['', '', 'Heya']]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiMtknjwq1yl",
        "outputId": "bf18b656-4915-4f2d-d59b-a514cc6dd07a"
      },
      "source": [
        "new_table = [['' for _ in range(n)] for _ in range(m)]\n",
        "\n",
        "new_table[1][2] = \"hello\"\n",
        "new_table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['', '', ''], ['', '', 'hello'], ['', '', ''], ['', '', '']]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iKnzfLfqwOq"
      },
      "source": [
        "18. Write code to initialize a two-dimensional array of sets called word_vowels and process a list of words, adding each word to word_vowels[l][v] where l is the length of the word and v is the number of vowels it contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOz_oF_Yq9vP"
      },
      "source": [
        "n = 10\n",
        "word_vowels = [[set() for _ in range(n)] for _ in range(n)]\n",
        "\n",
        "word_list = 'Write code to initialize an array and process a list of words'.split()\n",
        "for word in word_list:\n",
        "    l = len(word)\n",
        "    v = sum(1 for letter in word.lower() if letter in 'aeiou')\n",
        "    if l < n:                                # in case the length of word is larger than the array size n\n",
        "        word_vowels[l][v].add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_l1rqrzrCrM"
      },
      "source": [
        "19. Write a function novel10(text) that prints any word that appeared in the last 10% of a text that had not been encountered earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PotwdNrrbkr"
      },
      "source": [
        "\n",
        "def novel10(text):\n",
        "    novels = set()\n",
        "    text_list = text.split()\n",
        "    text_len = len(text_list)\n",
        "    for word in text_list[int(0.9 * text_len):]:\n",
        "        if word not in text_list[:int(0.9 * text_len)]:\n",
        "            novels.add(word)\n",
        "    for word in novels:\n",
        "        print(word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnBceBNMrha9"
      },
      "source": [
        "\n",
        "20. Write a program that takes a sentence expressed as a single string, splits it and counts up the words. Get it to print out each word and the word's frequency, one per line, in alphabetical order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11F2ulkzrCRW",
        "outputId": "b6594ee3-9cf5-480f-d9c3-497260b98b79"
      },
      "source": [
        "\n",
        "def split_and_word_freq(sent):\n",
        "    # splits = re.findall(r'\\w+', sent)\n",
        "    splits = re.findall(r\"\\w+(?:[-']\\w+)*\", sent)        # dealing with phrases like it's, warm-hearted\n",
        "\n",
        "    # create the word frequency using dictionary\n",
        "    word_freq = {}\n",
        "    for word in splits:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] = word_freq[word] + 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "    \n",
        "    # print word's frequency in alphabetical order\n",
        "    for key in sorted(word_freq.keys()):\n",
        "        print(key + ': ' + str(word_freq[key]))\n",
        "    \n",
        "sent = \"Text linguistics refers to a form of discourse analysisa method of studying written or spoken languagethat is concerned with the description and analysis of extended texts (those beyond the level of the single sentence). A text can be any example of written or spoken language, from something as complex as a book or legal document to something as simple as the body of an email or the words on the back of a cereal box.\"\n",
        "split_and_word_freq(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: 1\n",
            "Text: 1\n",
            "a: 4\n",
            "an: 1\n",
            "analysis: 2\n",
            "and: 1\n",
            "any: 1\n",
            "as: 4\n",
            "back: 1\n",
            "be: 1\n",
            "beyond: 1\n",
            "body: 1\n",
            "book: 1\n",
            "box: 1\n",
            "can: 1\n",
            "cereal: 1\n",
            "complex: 1\n",
            "concerned: 1\n",
            "description: 1\n",
            "discourse: 1\n",
            "document: 1\n",
            "email: 1\n",
            "example: 1\n",
            "extended: 1\n",
            "form: 1\n",
            "from: 1\n",
            "is: 1\n",
            "language: 2\n",
            "legal: 1\n",
            "level: 1\n",
            "linguistics: 1\n",
            "method: 1\n",
            "of: 7\n",
            "on: 1\n",
            "or: 4\n",
            "refers: 1\n",
            "sentence: 1\n",
            "simple: 1\n",
            "single: 1\n",
            "something: 2\n",
            "spoken: 2\n",
            "studying: 1\n",
            "text: 1\n",
            "texts: 1\n",
            "that: 1\n",
            "the: 6\n",
            "those: 1\n",
            "to: 2\n",
            "with: 1\n",
            "words: 1\n",
            "written: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lvNuT51rv4j"
      },
      "source": [
        "\n",
        "21. Read up on Gematria, a method for assigning numbers to words, and for mapping between words having the same number to discover the hidden meaning of texts (http://en.wikipedia.org/wiki/Gematria, http://essenes.net/gemcal.htm).\n",
        "a. Write a function gematria() that sums the numerical values of the letters of a word, according to the letter values in letter_vals :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UQoAXAOsfhU"
      },
      "source": [
        ">>> letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
        "... 'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
        "... 'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-g6DTCOsQse"
      },
      "source": [
        "def gematria(word):\n",
        "    value = 0\n",
        "    letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
        "                  'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
        "                  'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\n",
        "    for c in word:\n",
        "        if c in letter_vals:\n",
        "            value += letter_vals[c]\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScadY791sywU"
      },
      "source": [
        "22. Process a corpus (e.g. nltk.corpus.state_union ) and for each document, count how many of its words have the number 666."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy9qr3zgs6Dd",
        "outputId": "bf8bcd03-2b8c-4b92-de00-7551715931b6"
      },
      "source": [
        "for fileid in nltk.corpus.state_union.fileids():\n",
        "    cnt = 0\n",
        "    for word in nltk.corpus.state_union.words(fileid):\n",
        "        if word.isalpha() and gematria(word.lower()) == 666:\n",
        "            cnt += 1\n",
        "    print(fileid, cnt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1945-Truman.txt 2\n",
            "1946-Truman.txt 13\n",
            "1947-Truman.txt 0\n",
            "1948-Truman.txt 2\n",
            "1949-Truman.txt 2\n",
            "1950-Truman.txt 1\n",
            "1951-Truman.txt 0\n",
            "1953-Eisenhower.txt 1\n",
            "1954-Eisenhower.txt 6\n",
            "1955-Eisenhower.txt 3\n",
            "1956-Eisenhower.txt 1\n",
            "1957-Eisenhower.txt 2\n",
            "1958-Eisenhower.txt 5\n",
            "1959-Eisenhower.txt 1\n",
            "1960-Eisenhower.txt 5\n",
            "1961-Kennedy.txt 0\n",
            "1962-Kennedy.txt 11\n",
            "1963-Johnson.txt 0\n",
            "1963-Kennedy.txt 5\n",
            "1964-Johnson.txt 1\n",
            "1965-Johnson-1.txt 0\n",
            "1965-Johnson-2.txt 0\n",
            "1966-Johnson.txt 0\n",
            "1967-Johnson.txt 2\n",
            "1968-Johnson.txt 3\n",
            "1969-Johnson.txt 0\n",
            "1970-Nixon.txt 0\n",
            "1971-Nixon.txt 1\n",
            "1972-Nixon.txt 0\n",
            "1973-Nixon.txt 1\n",
            "1974-Nixon.txt 0\n",
            "1975-Ford.txt 0\n",
            "1976-Ford.txt 3\n",
            "1977-Ford.txt 0\n",
            "1978-Carter.txt 1\n",
            "1979-Carter.txt 2\n",
            "1980-Carter.txt 0\n",
            "1981-Reagan.txt 4\n",
            "1982-Reagan.txt 0\n",
            "1983-Reagan.txt 2\n",
            "1984-Reagan.txt 1\n",
            "1985-Reagan.txt 1\n",
            "1986-Reagan.txt 1\n",
            "1987-Reagan.txt 1\n",
            "1988-Reagan.txt 2\n",
            "1989-Bush.txt 1\n",
            "1990-Bush.txt 2\n",
            "1991-Bush-1.txt 0\n",
            "1991-Bush-2.txt 0\n",
            "1992-Bush.txt 3\n",
            "1993-Clinton.txt 1\n",
            "1994-Clinton.txt 2\n",
            "1995-Clinton.txt 1\n",
            "1996-Clinton.txt 2\n",
            "1997-Clinton.txt 1\n",
            "1998-Clinton.txt 4\n",
            "1999-Clinton.txt 1\n",
            "2000-Clinton.txt 3\n",
            "2001-GWBush-1.txt 1\n",
            "2001-GWBush-2.txt 0\n",
            "2002-GWBush.txt 0\n",
            "2003-GWBush.txt 3\n",
            "2004-GWBush.txt 2\n",
            "2005-GWBush.txt 2\n",
            "2006-GWBush.txt 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjv4BPp1tJ2h"
      },
      "source": [
        "23. Write a function decode() to process a text, randomly replacing words with their Gematria equivalents, in order to discover the \"hidden meaning\" of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaJIXjwTtMeW"
      },
      "source": [
        "def decode(text):\n",
        "    gematrias = {}\n",
        "    splits = text.split()\n",
        "\n",
        "    # create dictionary \n",
        "    # key: gematria value\n",
        "    # value: list of words with that gematria value\n",
        "    for word in splits:\n",
        "        if gematria(word) in gematrias:\n",
        "            gematrias[gematria(word)].add(word)\n",
        "        else:\n",
        "            gematrias[gematria(word)] = [word]\n",
        "            \n",
        "    for i in range(len(splits)):\n",
        "        splits[i] = random.choice(gematrias[gematria(word)])\n",
        "    \n",
        "    return ' '.join(splits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY6xn2qctaSu"
      },
      "source": [
        "24. Write a function shorten(text, n) to process a text, omitting the n most frequently occurring words of the text. How readable is it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GqUZLNk0tsIM",
        "outputId": "3c159134-3e1e-4eea-8c99-b8a860f78521"
      },
      "source": [
        "def shorten(text, n):\n",
        "    splits = re.findall(r\"\\w+(?:[-']\\w+)*\", text)\n",
        "    fdist = nltk.FreqDist(splits)\n",
        "    # most_freq = fdist.most_common(n)\n",
        "    # print(most_freq)\n",
        "    most_freq = []\n",
        "    for sample in fdist.most_common(n):\n",
        "        most_freq.append(sample[0])\n",
        "    for i in range(len(splits)):\n",
        "        if splits[i] in most_freq:\n",
        "            splits[i] = ''                     # in fact, the element should be deleted in the list\n",
        "    return ' '.join(splits)\n",
        "\n",
        "text = 'Write a function shorten(text, n) to process a text, omitting the n most frequently occurring words of the text. How readable is it?'\n",
        "shorten(text, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Write  function shorten   to process   omitting   most frequently occurring words of   How readable is it'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jn-kIIAt_T8"
      },
      "source": [
        "25. Write code to print out an index for a lexicon, allowing someone to look up words according to their meanings (or pronunciations; whatever properties are contained in lexical entries)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3cfHKsTuDhz"
      },
      "source": [
        "Not understand yet... Just exchange the key and value in nldk.corpus.cmudict?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc8ZTCQ-uH3V"
      },
      "source": [
        "26. Write a list comprehension that sorts a list of WordNet synsets for proximity to a given synset. For example, given the synsets minke_whale.n.01, orca.n.01, novel.n.01 , and tortoise.n.01 , sort them according to their shortest_path_distance() from right_whale.n.01 ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fTrz9nfuPfV",
        "outputId": "5101f694-d849-431c-9081-49118dcd316f"
      },
      "source": [
        "minke = wn.synset('minke_whale.n.01')\n",
        "orca = wn.synset('orca.n.01')\n",
        "novel = wn.synset('novel.n.01')\n",
        "tortoise = wn.synset('tortoise.n.01')\n",
        "right = wn.synset('right_whale.n.01')\n",
        "\n",
        "wn_list = [minke, orca, novel, tortoise]\n",
        "sorted(wn_list,\n",
        "       key=lambda x: right.shortest_path_distance(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('lesser_rorqual.n.01'),\n",
              " Synset('killer_whale.n.01'),\n",
              " Synset('tortoise.n.01'),\n",
              " Synset('novel.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FhEmI5Uufjj"
      },
      "source": [
        "27.  Write a function that takes a list of words (containing duplicates) and returns a list of words (with no duplicates) sorted by decreasing frequency. E.g. if the input list contained 10 instances of the word table and 9 instances of the word chair , then table would appear before chair in the output list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV2D8-eVu2MD"
      },
      "source": [
        "def decreasing_freq_with_no_duplicates(words):\n",
        "    wordset = set(words)\n",
        "    fdist = nltk.FreqDist(words)\n",
        "    return sorted(wordset,\n",
        "                 key=lambda x:fdist[x],\n",
        "                 reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWALFKRgvWeD",
        "outputId": "3e6ec121-503a-4a72-a90d-5c069a03d5f8"
      },
      "source": [
        "words = ['table'] * 10 + ['chair'] * 9\n",
        "decreasing_freq_with_no_duplicates(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['table', 'chair']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU7FbOs-vYWX"
      },
      "source": [
        "28. Write a function that takes a text and a vocabulary as its arguments and returns the set of words that appear in the text but not in the vocabulary. Both arguments can be represented as lists of strings. Can you do this in a single line, using set.difference() ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-UuDfMFvl3s"
      },
      "source": [
        "def diff(text, vocab):\n",
        "    return set(text).difference(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxEUzIu2vs8R",
        "outputId": "e9dfa43a-5616-4ca6-d38c-03bb57ccf3f9"
      },
      "source": [
        "text = 'a text and a vocabulary'.split()\n",
        "vocab = 'a vocabulary'.split()\n",
        "diff(text, vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and', 'text'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCaDq8xOv2zr"
      },
      "source": [
        "29.  Import the itemgetter() function from the operator module in Python's standard library (i.e. from operator import itemgetter ). Create a list words containing several words. Now try calling: sorted(words, key=itemgetter(1)) , and sorted(words, key=itemgetter(-1)) . Explain what itemgetter() is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qky8WhVUv8Vt",
        "outputId": "208f2dec-711c-45b5-dd2e-ac0065201d5f"
      },
      "source": [
        "\n",
        "words = 'list words containing several words'.split()\n",
        "sorted(words, key=itemgetter(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['several', 'list', 'words', 'containing', 'words']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-6YzP3_wVa0"
      },
      "source": [
        "30. Write a recursive function lookup(trie, key) that looks up a key in a trie, and returns the value it finds. Extend the function to return a word when it is uniquely determined by its prefix (e.g. vanguard is the only word that starts with vang- , so lookup(trie, 'vang') should return the same thing as lookup(trie, 'vanguard')) ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi4EEm7GwG-0"
      },
      "source": [
        "def insert(trie, key, value):\n",
        "    if key:\n",
        "        first, rest = key[0], key[1:]\n",
        "        if first not in trie:\n",
        "            trie[first] = {}\n",
        "        insert(trie[first], rest, value)\n",
        "    else:\n",
        "        trie['value'] = value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQr6nqpnwrvh"
      },
      "source": [
        "31.  Read up on \"keyword linkage\" (chapter 5 of (Scott & Tribble, 2006)). Extract keywords from NLTK's Shakespeare Corpus and using the NetworkX package, plot keyword linkage networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLEWiajJwzv3"
      },
      "source": [
        "Scott & Tribble, 2006 is not available. And I haven't instaled NetworkX neither."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuLxDKMQw2QY"
      },
      "source": [
        "32. Read about string edit distance and the Levenshtein Algorithm. Try the implementation provided in nltk.edit_distance(). In what way is this using dynamic programming? Does it use the bottom-up or top-down approach? [See also http://norvig.com/spell-correct.html]"
      ]
    }
  ]
}